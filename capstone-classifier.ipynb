{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import dill\n",
    "import base64\n",
    "import requests\n",
    "import PIL \n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import tarfile\n",
    "import os.path\n",
    "\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import Figure\n",
    "from shutil import rmtree\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlretrieve\n",
    "from tensorflow.image import resize\n",
    "from tensorflow.data import Dataset \n",
    "from tensorflow.keras import Model, Sequential, Input\n",
    "from tensorflow.keras.optimizers import SGD \n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, smart_resize,array_to_img,\\\n",
    "    ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB1, EfficientNetB2, EfficientNetB3,\\\n",
    "    EfficientNetB4, EfficientNetB5, EfficientNetB6, EfficientNetB7\n",
    "from tensorflow.data.experimental import save, cardinality\n",
    "from tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomRotation, RandomContrast\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.distribute import get_strategy\n",
    "from tensorflow.keras.models import load_model\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_STORAGE=\"./capstonedump\"\n",
    "PHOTOS_SOURCE=f\"{MAIN_STORAGE}/photos\"\n",
    "LATENT_VECTORS_STORAGE=f\"{MAIN_STORAGE}/latents\"\n",
    "TFMODELS_STORAGE=f\"{MAIN_STORAGE}/tfmodels\"\n",
    "PLOTS_STORAGE=f\"{MAIN_STORAGE}/plots\"\n",
    "LISTING_SOURCE=f\"{MAIN_STORAGE}/listing-images\"\n",
    "CATEGORY=\"kitchen\"\n",
    "BATCH=(20, 30, 50)\n",
    "IMG_SIZE={\n",
    "    \"EfficientNetB0\":224,\n",
    "    \"EfficientNetB1\":240,\n",
    "    \"EfficientNetB2\":260,\n",
    "    \"EfficientNetB3\":300,\n",
    "    \"EfficientNetB4\":380,\n",
    "    \"EfficientNetB5\":456,\n",
    "    \"EfficientNetB6\":528,\n",
    "    \"EfficientNetB7\":600, \n",
    "    \"InceptionV3\":299\n",
    "} \n",
    "LATENT_EPOCHS=1\n",
    "N_CLASSES=3\n",
    "N_COLORS=3\n",
    "STRATEGY=get_strategy() \n",
    "OPTIMIZATION_SUBDATASETS=(\"training\", \"validation\")\n",
    "MODEL_SPEC=\"EfficientNetB5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip_imageset():\n",
    "    if os.path.isdir(\"./capstonedump/photos\"):\n",
    "        ! rm -R ./capstonedump/photos\n",
    "    ! tar -xf ./capstonedump/large-imageset.tar.gz -C ./capstonedump/\n",
    "unzip_imageset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process jpg jpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### imgprep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```python\n",
    "class ImgPrep: \n",
    "    def decode_base64(in_save, in_encription):\n",
    "            with open(in_save, 'wb') as outfile_:\n",
    "                stripped=re.sub('data:image/jpeg;base64,', '', in_encription)\n",
    "                decoded=base64.b64decode((stripped))\n",
    "                outfile_.write(decoded) \n",
    "            \n",
    "    def decript_gstatic(in_save, in_gstatic):\n",
    "        browser.get(in_gstatic)\n",
    "        soup=BeautifulSoup(browser.page_source, 'lxml')\n",
    "        encriptedimg=[img[attr]  \n",
    "                      for attr in ['src', 'data-src']  \n",
    "                      for img in soup.find_all('img', \n",
    "                                               {attr: True})]\n",
    "        urlretrieve(encriptedimg[0] , in_save)\n",
    "\n",
    "    def download(in_imglinks, in_category=\"lux-kitchen\"):\n",
    "        base64_save=f\"./capstonedump/base64-{in_category}\"+\"-{}.jpeg\"\n",
    "        gstatic_save=f\"./capstonedump/gstatic-{in_category}\"+\"-{}.jpg\"\n",
    "\n",
    "        [ImgPrep.decode_base64(base64_save.format(index), unit)\n",
    "         if \"data:image/jpeg;base64,/9j\"  \n",
    "         in unit \n",
    "         else urlretrieve(unit, gstatic_save.format(index))\n",
    "         for index, unit \n",
    "         in enumerate(luxlinks)]\n",
    "\n",
    "    def digitize(in_imgfiles):\n",
    "        return [img_to_array(load_img(unit)) \n",
    "                for unit in in_imgfiles]\n",
    "\n",
    "    def rescale(in_pixelstack, img_size = IMG_SIZE[0], in_save=\"./capstonedump/resized.pkd\"):\n",
    "        return [smart_resize(unit, size=(IMG_SIZE[0], IMG_SIZE[0])) \n",
    "                    for unit \n",
    "                    in digitized]\n",
    "    \n",
    "    def label_from_file(in_file):\n",
    "        possession_=(re\n",
    "                    .search(\n",
    "                        \"|\".join(LABELS.keys()),\n",
    "                        in_file) \n",
    "                    .group())\n",
    "        return LABELS[possession_] \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### first img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`imgfiles=os.listdir('./capstonedump/')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`jpgpaths=[\"./capstonedump/\"+file for file in os.listdir('./capstonedump/') if file.endswith('.jpg')]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`jpgloaded=load_img(jpgpaths[0])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`analog=img_to_array(jpgloaded)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`rescaled=smart_resize(analog, size=(224, 224))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`array_to_img(rescaled)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### tf preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Assume a set of jpgs and jpegs (w. labels)\n",
    "\n",
    "Preprocess imgs via tf:\n",
    "1. load pkd img links (base64 jpegs and gstatic jpg)\n",
    "2. convert into arrays ($N \\times K \\times K$)\n",
    "3. one hot encode labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### merge classes && make labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```\n",
    "['allsrc-luxurious-kitchen.pkd',\n",
    " 'allsrc-dilapidated-kitchen.pkd',\n",
    " 'allsrc-home-kitchen.pkd']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```python\n",
    "[os.remove(f\"./capstonedump/{unit}\")\n",
    " for unit in os.listdir(\"./capstonedump/\") \n",
    " if any(map(unit.__contains__, [\"jpg\", \"jpeg\"]))]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### arrayfy imgs put into labelled classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```python\n",
    "[os.mkdir(f\"{DATASET_SAVEDIR}{i}\") for i in range(N_CLASSES)]\n",
    "[os.mkdir(f\"{PHOTO_SAVEDIR}{i}/\") for i in range(N_CLASSES)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```python\n",
    "links_in_classes=[(dill.load(open(f\"{CORE_SAVEDIR}{file}\", \"rb\")),  \n",
    "                    ImgPrep.label_from_file(file))\n",
    "                        for file in os.listdir(CORE_SAVEDIR) \n",
    "                        if file.startswith(\"allsrc\")  \n",
    "                            and file.endswith(\".pkd\")]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```python\n",
    "[ImgPrep.decode_base64(f\"{PHOTO_SAVEDIR}{label}/{index}.jpeg\", unit)\n",
    "    if \"data:image/jpeg;base64,/9j\" in unit\n",
    "    else urlretrieve(unit, f\"{PHOTO_SAVEDIR}{label}/{index}.jpg\")\n",
    "    for links, label in links_in_classes\n",
    "    for index, unit in enumerate(links)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```python\n",
    "[np.save(f\"{DATASET_SAVEDIR}{label}/{index}.npy\",  \n",
    "        img_to_array(load_img(f\"{PHOTO_SAVEDIR}{label}/{file}\")),\n",
    "        allow_pickle=True)\n",
    "    for label in os.listdir(f\"{PHOTO_SAVEDIR}\")\n",
    "    for index, file in enumerate(os.listdir(f\"{PHOTO_SAVEDIR}{label}\"))]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```python \n",
    "[os.remove(f\"{CORE_SAVEDIR}{file}\") \n",
    " for file in os.listdir(CORE_SAVEDIR) \n",
    " if file.endswith(('.jpg', '.jpeg'))]\n",
    "labels_and_images()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```python\n",
    "class ImgDecoder:\n",
    "    def make_diretories():\n",
    "        [os.mkdir(f\"{DATASET_SAVEDIR}{i}\")  \n",
    "         for i in range(N_CLASSES)]\n",
    "        [os.mkdir(f\"{PHOTO_SAVEDIR}{i}/\") \n",
    "         for i in range(N_CLASSES)]\n",
    "        \n",
    "    def links_in_classes():\n",
    "        return [(dill.load(open(f\"{CORE_SAVEDIR}{file}\",  \n",
    "                                \"rb\")),  \n",
    "                 ImgPrep.label_from_file(file))\n",
    "                    for file  \n",
    "                    in os.listdir(CORE_SAVEDIR) \n",
    "                    if file.startswith(\"allsrc\")  \n",
    "                        and file.endswith(\".pkd\")]\n",
    "    \n",
    "    def decode_from_links():\n",
    "        [ImgPrep.decode_base64(f\"{PHOTO_SAVEDIR}{label}/{index}.jpeg\", unit)\n",
    "            if \"data:image/jpeg;base64,/9j\" \n",
    "            in unit\n",
    "            else urlretrieve(unit, f\"{PHOTO_SAVEDIR}{label}/{index}.jpg\")\n",
    "            for links, label  \n",
    "            in ImgDecoder.links_in_classes()\n",
    "            for index, unit  \n",
    "            in enumerate(links)]\n",
    "    \n",
    "    def run():\n",
    "        ImgDecoder.make_diretories()\n",
    "        ImgDecoder.decode_from_links()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load images && tf reprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def split_factory(**KWARGS):\n",
    "    return image_dataset_from_directory(\n",
    "                KWARGS[\"image_source\"], \n",
    "                labels=\"inferred\", \n",
    "                label_mode=\"int\",\n",
    "                class_names=list(map(str,  \n",
    "                                     range(N_CLASSES))),\n",
    "                color_mode=\"rgb\",\n",
    "                image_size=(KWARGS['image_size'],  \n",
    "                            KWARGS['image_size']),\n",
    "                subset=KWARGS[\"subset\"],\n",
    "                shuffle=True,\n",
    "                batch_size=KWARGS['batch_size'],\n",
    "                seed=1,\n",
    "                validation_split=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split={subset: \n",
    "           split_factory(image_source=PHOTOS_SOURCE,\n",
    "                         subset=subset, \n",
    "                         image_size=IMG_SIZE[MODEL_SPEC], \n",
    "                         batch_size=BATCH[0]) \n",
    "       for subset  \n",
    "       in OPTIMIZATION_SUBDATASETS} \n",
    "\n",
    "distributed={index: \n",
    "             STRATEGY.experimental_distribute_dataset(dataset)\n",
    "             for index, dataset in split.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```python\n",
    "Found 2823 files belonging to 3 classes.\n",
    "Using 2400 files for training.\n",
    "Found 2823 files belonging to 3 classes.\n",
    "Using 423 files for validation.\n",
    "CPU times: user 309 ms, sys: 51.3 ms, total: 360 ms\n",
    "Wall time: 316 ms\n",
    "    \n",
    "inceptionv3 ? b?\n",
    "```\n",
    "```python\n",
    "# include_top=False will discard avg_pool before prediction layer\n",
    "inception = tf.keras.applications.inception_v3.InceptionV3(include_top=True, input_shape=(299, 299, 3))\n",
    "inception = tf.keras.Model([inception.input], [inception.layers[-2].output]) # manually discard prediction layer\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def pretrained_factory(**KWARGS):\n",
    "    clear_session()\n",
    "    expert=EfficientNetB5(\n",
    "                include_top=True, \n",
    "                input_shape=(KWARGS[\"img_size\"],  \n",
    "                             KWARGS[\"img_size\"],  \n",
    "                             N_COLORS))\n",
    "    pretrained=tf.keras.Model([expert.input],\n",
    "                   [expert.layers[-KWARGS[\"num_layers_to_exclude\"]] \n",
    "                          .output])\n",
    "    pretrained.trainable=False\n",
    "    pipeline=Sequential()\n",
    "    pipeline.add(RandomFlip('horizontal', \n",
    "                            seed=1,\n",
    "                            input_shape=(KWARGS['img_size'], \n",
    "                                         KWARGS['img_size'], \n",
    "                                         N_COLORS)))\n",
    "    pipeline.add(RandomRotation(KWARGS['rotation_factor'], \n",
    "                                seed=1))\n",
    "    pipeline.add(RandomContrast(KWARGS['contrast_factor'], \n",
    "                                seed=1))\n",
    "    pipeline.add(pretrained)\n",
    "    pipeline.summary()\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_student=pretrained_factory(\n",
    "                     img_size=IMG_SIZE[MODEL_SPEC],\n",
    "                     num_layers_to_exclude=2,\n",
    "                     rotation_factor=0.1,\n",
    "                     contrast_factor=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def clean_directories_in(in_dir, **KWARGS):\n",
    "    if in_dir:\n",
    "        rmtree(in_dir)\n",
    "    os.mkdir(in_dir)\n",
    "    [os.mkdir(f\"{in_dir}/{subdirectory}\")  \n",
    "         for subdirectory \n",
    "         in KWARGS['subdir'] ]\n",
    "\n",
    "def calculate_latents(in_distributed_dataset, \n",
    "                      in_pretrained_model, \n",
    "                      **KWARGS\n",
    "                     ): \n",
    "    SAVEPATH_=\"{}/{}/vectors-per-step-{}.npy\"\n",
    "    with STRATEGY.scope():\n",
    "        for subset, dataset in in_distributed_dataset.items():\n",
    "            for _ in range(LATENT_EPOCHS):\n",
    "                for index, bundle in enumerate(\n",
    "                                         iter(\n",
    "                                             dataset)):\n",
    "                    try:\n",
    "                        latent_per_step=(in_pretrained_model \n",
    "                                         .predict(\n",
    "                                             tf.convert_to_tensor(bundle[0]), \n",
    "                                             verbose=0))\n",
    "                        clear_session()\n",
    "                        gc.collect()\n",
    "                    except tf.errors.InvalidArgumentError: \n",
    "                        pass\n",
    "                    with open(SAVEPATH_.format(KWARGS[\"latents_save_location\"],\n",
    "                                               subset, \n",
    "                                               index),  \n",
    "                              \"wb\") \\\n",
    "                        as fout_:\n",
    "                        np.save(fout_,\n",
    "                                latent_per_step,\n",
    "                                allow_pickle=True) \n",
    "                    del latent_per_step\n",
    "\n",
    "def compress_latents(in_path, out_path):\n",
    "    with tarfile.open(out_path, \"w:gz\") as tar_:\n",
    "        tar_.add(in_path, \n",
    "                 arcname=os.path.basename(in_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "clean_directories_in(LATENT_VECTORS_STORAGE, \n",
    "                     subdir=OPTIMIZATION_SUBDATASETS)\n",
    "\n",
    "calculate_latents(distributed,\n",
    "                  transfer_student,\n",
    "                  latents_save_location=LATENT_VECTORS_STORAGE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compress_latents(LATENT_VECTORS_STORAGE, f\"{MAIN_STORAGE}/latents-large-{MODEL_SPEC}.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def load_latent_vectors(in_load_path, \n",
    "                        **KWARGS\n",
    "                       ):\n",
    "    unsorted=[(int(re.split(\"\\-|\\.\", file)[-2]), \n",
    "               np.load(f'{in_load_path}/{file}'))\n",
    "              for file \n",
    "              in os.listdir(in_load_path)]\n",
    "    ordered=sorted(unsorted,  \n",
    "                   key = lambda index_dataset: \n",
    "                             index_dataset[0])\n",
    "    \n",
    "    stacked=(np.vstack(\n",
    "                [dataset  \n",
    "                 for index, dataset  \n",
    "                 in ordered]))\n",
    "    return stacked \n",
    "def one_hot_labels(in_label_images):\n",
    "    \"\"\"\n",
    "    :in_label_images: \n",
    "        BatchDataset: image arrays (batch, pixels, pixels, colors) \n",
    "            && labels\n",
    "    \"\"\"\n",
    "    unpacked_=list(\n",
    "                 chain(\n",
    "                     *[labels \n",
    "                       for images, labels \n",
    "                       in in_label_images])) \n",
    "    return tf.one_hot(unpacked_,  \n",
    "                      depth=N_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "try:\n",
    "    latents_and_labels\n",
    "    del latents_and_labels\n",
    "    gc.collect()\n",
    "except NameError:\n",
    "    pass \n",
    "\n",
    "latents_and_labels={subset:\n",
    "                    (load_latent_vectors(f\"{LATENT_VECTORS_STORAGE}/{subset}/\"),\n",
    "                     one_hot_labels(split[subset]))\n",
    "                         for subset in OPTIMIZATION_SUBDATASETS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_facotory(**KWARGS):\n",
    "    classifier=Sequential()\n",
    "    classifier.add(\n",
    "        Dense(\n",
    "            N_CLASSES, \n",
    "            name=\"softmax\",\n",
    "            activation=\"softmax\",\n",
    "            input_dim=KWARGS['input_dim']))\n",
    "\n",
    "    classifier.compile(\n",
    "        optimizer=SGD(lr=0.0001,  \n",
    "                      momentum=1.,  \n",
    "                      nesterov=True),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "CLASSIFICATION_EPOCHS=1_000\n",
    "\n",
    "clear_session()\n",
    "classifier=classifier_facotory(\n",
    "               input_dim=latent_dimension)\n",
    "history=(classifier \n",
    "         .fit( \n",
    "            latents_and_labels['training'][0],  \n",
    "            latents_and_labels['training'][1], \n",
    "            validation_data=tuple( \n",
    "                                latents_and_labels[\"validation\"]),\n",
    "            epochs=CLASSIFICATION_EPOCHS, \n",
    "            batch_size=BATCH[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.mkdir(f\"{TFMODELS_STORAGE}/{MODEL_SPEC}\")\n",
    "classifier.save(f\"{TFMODELS_STORAGE}/{MODEL_SPEC}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracies(in_train, \n",
    "                    in_val, \n",
    "                    **KWARGS):\n",
    "    acc_over_epochs=plt.figure()\n",
    "    acc_over_epochs.patch.set_facecolor('white')\n",
    "    acc_over_epochs.patch.alpha=.7\n",
    "    drawer=acc_over_epochs.add_subplot(111, xlabel=\"epochs\", ylabel=\"accuracy\")\n",
    "    \n",
    "    curves=[drawer.plot(history, label=subset)  \n",
    "            for subset, history  \n",
    "            in zip(OPTIMIZATION_SUBDATASETS, [in_train, in_val])]\n",
    "    handles, labels=drawer.get_legend_handles_labels()\n",
    "    plt.legend(handles, labels)\n",
    "    if (save_dir_:=KWARGS[\"saveas\"]):\n",
    "        acc_over_epochs.savefig(save_dir_)\n",
    "    return acc_over_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_over_epochs=plot_accuracies(\n",
    "                    history.history[\"accuracy\"], \n",
    "                    history.history[\"val_accuracy\"],\n",
    "                    saveas=f\"{PLOTS_STORAGE}/temp2\")\n",
    "acc_over_epochs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_responses=np.argmax(\n",
    "                            classifier \n",
    "                            .predict( \n",
    "                                latents_and_labels[\"validation\"][0]), \n",
    "                            axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(classifier_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tag listing kitchens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "(depricated)\n",
    "Zillow profiles too hard to scrape\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_session()\n",
    "transfer_student=pretrained_factory(\n",
    "                     img_size=IMG_SIZE[MODEL_SPEC],\n",
    "                     num_layers_to_exclude=2,\n",
    "                     rotation_factor=0.1,\n",
    "                     contrast_factor=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "address_images=[(file.split('.')[0], \n",
    " tf.image.resize(\n",
    "              img_to_array(\n",
    "              Image.open(f\"{LISTING_SOURCE}/{file}\")),\n",
    "              [IMG_SIZE[MODEL_SPEC], \n",
    "               IMG_SIZE[MODEL_SPEC]]))\n",
    " for file \n",
    " in os.listdir(f\"{LISTING_SOURCE}/\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "latent=transfer_student.predict(tf.stack([duo[1] for duo in address_images]), verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{MAIN_STORAGE}/latents-listings-and-addresses-{MODEL_SPEC}.pkd\", \"wb\") as writefile:\n",
    "    dill.dump(\n",
    "        ([duo[0] for duo in address_images],\n",
    "         latent),\n",
    "        writefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier=classifier_facotory(input_dim=latent.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier=load_model(TFMODELS_STORAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifier.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predictions=np.argmax(classifier.predict(tf.convert_to_tensor(latent)),\n",
    "                            axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address_ratings={k:v \n",
    "                 for k,v in zip([duo[0] \n",
    "                                 for duo in address_images], model_predictions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address_ratings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
