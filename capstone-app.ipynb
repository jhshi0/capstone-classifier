{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture installation_results\n",
    "def install_packages():\n",
    "    ! pip install geopy\n",
    "    ! pip install geopandas\n",
    "    ! pip install folium\n",
    "install_packages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt \n",
    "import geopy\n",
    "import geopandas\n",
    "\n",
    "import pandas as pd\n",
    "import folium\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import dill\n",
    "import base64\n",
    "import requests\n",
    "import PIL \n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import tarfile\n",
    "import os.path\n",
    "import shutil\n",
    "\n",
    "from time import sleep \n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import Figure\n",
    "from shutil import rmtree\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlretrieve\n",
    "from tensorflow.image import resize\n",
    "from tensorflow.data import Dataset \n",
    "from tensorflow.keras import Model, Sequential, Input\n",
    "from tensorflow.keras.optimizers import SGD \n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, smart_resize,array_to_img,\\\n",
    "    ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB1, EfficientNetB2, EfficientNetB3,\\\n",
    "    EfficientNetB4, EfficientNetB5, EfficientNetB6, EfficientNetB7\n",
    "from tensorflow.data.experimental import save, cardinality\n",
    "from tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomRotation, RandomContrast\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.distribute import get_strategy\n",
    "from tensorflow.keras.models import load_model\n",
    "from itertools import chain\n",
    "from geopy import Nominatim\n",
    "from folium import IFrame\n",
    "from geopy.extra.rate_limiter import RateLimiter \n",
    "from pandas.io.json import read_json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 400)\n",
    "\n",
    "MAIN_STORAGE=\"/home/jovyan/jhshi/capstonedump\"\n",
    "PHOTOS_SOURCE=f\"{MAIN_STORAGE}/photos\"\n",
    "LATENT_VECTORS_STORAGE=f\"{MAIN_STORAGE}/latents\"\n",
    "TFMODELS_STORAGE=f\"{MAIN_STORAGE}/tfmodels\"\n",
    "PLOTS_STORAGE=f\"{MAIN_STORAGE}/plots\"\n",
    "LISTINGS_STORAGE=f\"/home/jovyan/jhshi/capstoneapp/downloads\"\n",
    "APP_DUMPATH=\"/home/jovyan/jhshi/capstoneapp\"\n",
    "CATEGORY=\"kitchen\"\n",
    "BATCH=(20, 30, 50)\n",
    "IMG_SIZE={\n",
    "    \"EfficientNetB0\":224,\n",
    "    \"EfficientNetB1\":240,\n",
    "    \"EfficientNetB2\":260,\n",
    "    \"EfficientNetB3\":300,\n",
    "    \"EfficientNetB4\":380,\n",
    "    \"EfficientNetB5\":456,\n",
    "    \"EfficientNetB6\":528,\n",
    "    \"EfficientNetB7\":600, \n",
    "    \"InceptionV3\":299\n",
    "} \n",
    "LATENT_EPOCHS=1\n",
    "N_CLASSES=3\n",
    "N_COLORS=3\n",
    "STRATEGY=get_strategy() \n",
    "OPTIMIZATION_SUBDATASETS=(\"training\", \"validation\")\n",
    "MODEL_SPEC=\"EfficientNetB5\"\n",
    "LOCATOR=Nominatim(user_agent=\"myGeoendoer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tar -xvf addresses-and-images.tar.gz -C /home/jovyan/jhshi/capstoneapp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## train some images TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def split_factory(**KWARGS):\n",
    "    return image_dataset_from_directory(\n",
    "                KWARGS[\"image_source\"], \n",
    "                labels=\"inferred\", \n",
    "                label_mode=\"int\",\n",
    "                class_names=list(map(str,  \n",
    "                                     range(N_CLASSES))),\n",
    "                color_mode=\"rgb\",\n",
    "                image_size=(KWARGS['image_size'],  \n",
    "                            KWARGS['image_size']),\n",
    "                subset=KWARGS[\"subset\"],\n",
    "                shuffle=True,\n",
    "                batch_size=KWARGS['batch_size'],\n",
    "                seed=1,\n",
    "                validation_split=0.20)\n",
    "\n",
    "def pretrained_factory(**KWARGS):\n",
    "    clear_session()\n",
    "    expert=EfficientNetB5(\n",
    "                include_top=True, \n",
    "                input_shape=(KWARGS[\"img_size\"],  \n",
    "                             KWARGS[\"img_size\"],  \n",
    "                             N_COLORS))\n",
    "    pretrained=tf.keras.Model([expert.input],\n",
    "                   [expert.layers[-KWARGS[\"num_layers_to_exclude\"]] \n",
    "                          .output])\n",
    "    pretrained.trainable=False\n",
    "    pipeline=Sequential()\n",
    "    pipeline.add(RandomFlip('horizontal', \n",
    "                            seed=1,\n",
    "                            input_shape=(KWARGS['img_size'], \n",
    "                                         KWARGS['img_size'], \n",
    "                                         N_COLORS)))\n",
    "    pipeline.add(RandomRotation(KWARGS['rotation_factor'], \n",
    "                                seed=1))\n",
    "    pipeline.add(RandomContrast(KWARGS['contrast_factor'], \n",
    "                                seed=1))\n",
    "    pipeline.add(pretrained)\n",
    "    pipeline.summary()\n",
    "    return pipeline\n",
    "\n",
    "def load_latent_vectors(in_load_path, \n",
    "                        **KWARGS\n",
    "                       ):\n",
    "    unsorted=[(int(re.split(\"\\-|\\.\", file)[-2]), \n",
    "               np.load(f'{in_load_path}/{file}'))\n",
    "              for file \n",
    "              in os.listdir(in_load_path)]\n",
    "    ordered=sorted(unsorted,  \n",
    "                   key = lambda index_dataset: \n",
    "                             index_dataset[0])\n",
    "    \n",
    "    stacked=(np.vstack(\n",
    "                [dataset  \n",
    "                 for index, dataset  \n",
    "                 in ordered]))\n",
    "    return stacked \n",
    "\n",
    "def one_hot_labels(in_label_images):\n",
    "    \"\"\"\n",
    "    :in_label_images: \n",
    "        BatchDataset: image arrays (batch, pixels, pixels, colors) \n",
    "            && labels\n",
    "    \"\"\"\n",
    "    unpacked_=list(\n",
    "                 chain(\n",
    "                     *[labels \n",
    "                       for images, labels \n",
    "                       in in_label_images])) \n",
    "    return tf.one_hot(unpacked_,  \n",
    "                      depth=N_CLASSES)\n",
    "\n",
    "def clean_directories_in(in_dir, **KWARGS):\n",
    "    if in_dir:\n",
    "        rmtree(in_dir)\n",
    "    os.mkdir(in_dir)\n",
    "    [os.mkdir(f\"{in_dir}/{subdirectory}\")  \n",
    "         for subdirectory \n",
    "         in KWARGS['subdir'] ]\n",
    "\n",
    "def classifier_facotory(**KWARGS):\n",
    "    classifier=Sequential()\n",
    "    classifier.add(\n",
    "        Dense(\n",
    "            N_CLASSES, \n",
    "            name=\"softmax\",\n",
    "            activation=\"softmax\",\n",
    "            input_dim=KWARGS['input_dim']))\n",
    "\n",
    "    classifier.compile(\n",
    "        optimizer=SGD(lr=0.0001,  \n",
    "                      momentum=1.,  \n",
    "                      nesterov=True),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    return classifier\n",
    "\n",
    "def clean_directories_in(in_dir, **KWARGS):\n",
    "    if in_dir:\n",
    "        rmtree(in_dir)\n",
    "    os.mkdir(in_dir)\n",
    "    [os.mkdir(f\"{in_dir}/{subdirectory}\")  \n",
    "         for subdirectory \n",
    "         in KWARGS['subdir'] ]\n",
    "\n",
    "def calculate_latents(in_distributed_dataset, \n",
    "                      in_pretrained_model, \n",
    "                      **KWARGS\n",
    "                     ): \n",
    "    SAVEPATH_=\"{}/{}/vectors-per-step-{}.npy\"\n",
    "    with STRATEGY.scope():\n",
    "        for subset, dataset in in_distributed_dataset.items():\n",
    "            for _ in range(LATENT_EPOCHS):\n",
    "                for index, bundle in enumerate(\n",
    "                                         iter(\n",
    "                                             dataset)):\n",
    "                    try:\n",
    "                        latent_per_step=(in_pretrained_model \n",
    "                                         .predict(\n",
    "                                             tf.convert_to_tensor(bundle[0]), \n",
    "                                             verbose=0))\n",
    "                        clear_session()\n",
    "                        gc.collect()\n",
    "                    except tf.errors.InvalidArgumentError: \n",
    "                        pass\n",
    "                    with open(SAVEPATH_.format(KWARGS[\"latents_save_location\"],\n",
    "                                               subset, \n",
    "                                               index),  \n",
    "                              \"wb\") \\\n",
    "                        as fout_:\n",
    "                        np.save(fout_,\n",
    "                                latent_per_step,\n",
    "                                allow_pickle=True) \n",
    "                    del latent_per_step\n",
    "\n",
    "def compress_latents(in_path, out_path):\n",
    "    with tarfile.open(out_path, \"w:gz\") as tar_:\n",
    "        tar_.add(in_path, \n",
    "                 arcname=os.path.basename(in_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "split={subset: \n",
    "           split_factory(image_source=PHOTOS_SOURCE,\n",
    "                         subset=subset, \n",
    "                         image_size=IMG_SIZE[MODEL_SPEC], \n",
    "                         batch_size=BATCH[0]) \n",
    "       for subset  \n",
    "       in OPTIMIZATION_SUBDATASETS} \n",
    "\n",
    "distributed={index: \n",
    "             STRATEGY.experimental_distribute_dataset(dataset)\n",
    "             for index, dataset in split.items()}\n",
    "\n",
    "transfer_student=pretrained_factory(\n",
    "                     img_size=IMG_SIZE[MODEL_SPEC],\n",
    "                     num_layers_to_exclude=2,\n",
    "                     rotation_factor=0.1,\n",
    "                     contrast_factor=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    latents_and_labels\n",
    "    del latents_and_labels\n",
    "    gc.collect()\n",
    "except NameError:\n",
    "    pass \n",
    "\n",
    "latents_and_labels={subset:\n",
    "                    (load_latent_vectors(f\"{LATENT_VECTORS_STORAGE}/{subset}/\"),\n",
    "                     one_hot_labels(split[subset]))\n",
    "                         for subset in OPTIMIZATION_SUBDATASETS}\n",
    "latent_dimension=latents_and_labels[\"training\"][0].shape[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "classifier=load_model(f\"{TFMODELS_STORAGE}/{MODEL_SPEC}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_listings=[f\"{LISTINGS_STORAGE}/{file}\"  \n",
    "              for file \n",
    "              in os.listdir(LISTINGS_STORAGE) \n",
    "              if file.endswith((\".jpeg\", \".jpg\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "def get_index_tags(in_image):\n",
    "    \"\"\"\n",
    "    :in_image: jpg or jpeg \n",
    "    note name of the image reflects the indexing in the listing imageset\n",
    "    predicts quality from the in_image\n",
    "    \"\"\"\n",
    "    rescaled= resize(img_to_array(Image.open(in_image)), \n",
    "                     [IMG_SIZE[MODEL_SPEC], IMG_SIZE[MODEL_SPEC]])\n",
    "    index=re.split(r\"\\/|\\.\", in_image)[-2]\n",
    "    reshaped=tf.convert_to_tensor(np.array([rescaled,]))\n",
    "    latent_vector=transfer_student.predict(reshaped, verbose=0)\n",
    "    tag=np.argmax(classifier.predict(latent_vector), axis=1)[0]\n",
    "    clear_session()\n",
    "    return (int(index), tag)\n",
    "    \n",
    "index_tags=[get_index_tags(file) for file in all_listings]\n",
    "ordered=sorted(index_tags, key=lambda x: x[0])\n",
    "indiced_tags=[pair[1] for pair in ordered]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    os.remove(f\"{APP_DUMPATH}/index_and_tags.pkd\")\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "with open(f\"{APP_DUMPATH}/index_and_tags.pkd\", \"wb\") as writefile:\n",
    "    dill.dump(indiced_tags, writefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## altair a single property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "property_address=\"301 Van Ness Ave, San Francisco, CA 94102\"\n",
    "locator=Nominatim(user_agent=\"myGeoendoer\")\n",
    "location=locator.geocode(property_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "address_coordinates=pd.DataFrame(\n",
    "    {\n",
    "        \"address\": [property_address],\n",
    "        \"longitude\":[location.longitude],\n",
    "        \"latitude\":[location.latitude]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "displayer=folium.Map(\n",
    "                    location=[location.latitude, \n",
    "                              location.longitude], \n",
    "                    zoom_start=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "folium.Marker(\n",
    "    location=[location.latitude, \n",
    "              location.longitude],\n",
    "    popup=property_address\n",
    ").add_to(displayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "displayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## alta√Ør property listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preload_listing_profiles(**KWARGS):\n",
    "    with open(KWARGS[\"in_addresses_loadpath\"], \"rb\") as readfile:\n",
    "        indices_and_addresses=dill.load(readfile)\n",
    "    sorted_addresses=[addr for index, addr in indices_and_addresses]\n",
    "    images=[\"{}/{}\".format(KWARGS[\"in_images_loadpath\"], file)\n",
    "            for file \n",
    "            in os.listdir(KWARGS[\"in_images_loadpath\"])\n",
    "            if file.endswith((\".jpeg\", \".jpg\"))]\n",
    "    sorted_images=sorted(\n",
    "        images,\n",
    "        key=\n",
    "        lambda x:\n",
    "        int(re.split(r\"\\/|\\.\", x)[-2])\n",
    "    )\n",
    "    with open(KWARGS[\"in_tags_loadpath\"], \"rb\") as readfile:\n",
    "        indiced_tags=dill.load(readfile)\n",
    "    return {\n",
    "        \"out_sorted_tags\":indiced_tags,\n",
    "        \"out_sorted_images\":sorted_images,\n",
    "        \"out_sorted_addresses\":sorted_addresses\n",
    "    }\n",
    "\n",
    "def make_listing_profiles(**KWARGS):\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"tags\":KWARGS[\"in_sorted_tags\"],\n",
    "            \"images\": KWARGS[\"in_sorted_images\"],\n",
    "            \"addresses\":KWARGS[\"in_sorted_addresses\"]\n",
    "        }\n",
    "    )\n",
    "\n",
    "preloaded=preload_listing_profiles(\n",
    "            in_addresses_loadpath=f\"{LISTINGS_STORAGE}/address_index.pkd\" ,\n",
    "            in_tags_loadpath=f\"{APP_DUMPATH}/index_and_tags.pkd\",\n",
    "            in_images_loadpath=LISTINGS_STORAGE)\n",
    "\n",
    "profile_with_null_addresses=make_listing_profiles(\n",
    "            in_sorted_tags=preloaded[\"out_sorted_tags\"],\n",
    "            in_sorted_images=preloaded[\"out_sorted_images\"],\n",
    "            in_sorted_addresses=preloaded[\"out_sorted_addresses\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{APP_DUMPATH}/listing_profiles.pkd\", \"wb\") as writefile:\n",
    "    dill.dump(profile, writefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POPUP_HTML_SCRIPT='''\n",
    "    <img src=\"data:image/png;base64,{}\">\n",
    "    <p> address: {} </p>\n",
    "    <p> room quality: {} </p>\n",
    "'''\n",
    "ROOM_INDEX_TO_QUALITY={\n",
    "    0:\"poor\",\n",
    "    1:\"normal\",\n",
    "    2:\"luxurious\"\n",
    "}\n",
    "POPUP_FRAME_SIZE=300\n",
    "def make_popup(**KWARGS):\n",
    "    with open(KWARGS[\"in_image_source\"], \"rb\") as readfile:\n",
    "        encoded_image=base64.b64encode(readfile.read())\n",
    "        image_tag=POPUP_HTML_SCRIPT.format\n",
    "        in_frame_=IFrame(image_tag(encoded_image.decode('UTF-8'),\n",
    "                                  KWARGS[\"in_location\"],\n",
    "                                  ROOM_INDEX_TO_QUALITY[KWARGS[\"in_tag\"]]\n",
    "                                 ),\n",
    "                        width=POPUP_FRAME_SIZE, height=POPUP_FRAME_SIZE)\n",
    "        return folium.Popup(in_frame_, max_width=POPUP_FRAME_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_location_on_map(in_map, **KWARGS):\n",
    "    marker_popup=make_popup(\n",
    "                    in_location=KWARGS[\"in_location\"],\n",
    "                    in_tag=KWARGS[\"in_tag\"],\n",
    "                    in_image_source=KWARGS[\"in_image_source\"])\n",
    "    folium.Marker(location= KWARGS[\"in_coordinates\"],\n",
    "                  popup=marker_popup\n",
    "                 ).add_to(in_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture profile_printout\n",
    "def trim_address(in_address):\n",
    "    split=re.split(r\"\\,|MLS\",in_address)\n",
    "    trim_street_ends=re.sub(r\"APT\\s+[0-9]+|[\\/|\\#][\\s,a-z,A-Z,0-9]+|[UNIT|SPACE]\\s[0-9]+\", \"\", split[0])\n",
    "    return f\"{trim_street_ends},CA\"\n",
    "\n",
    "profile=profile_with_null_addresses[~profile_with_null_addresses\n",
    "                                    .addresses\n",
    "                                    .isna()]\n",
    "\n",
    "num_address_tokens=(profile\n",
    " .addresses\n",
    " .apply(\n",
    "     lambda x: re.split(r\"\\,|MLS\", x))\n",
    " .str.len())\n",
    "\n",
    "profile_valid_addresses=profile[num_address_tokens>2] \n",
    "\n",
    "profile_valid_addresses['truncated_addresses']=profile_valid_addresses.addresses.apply(\n",
    "    lambda x: trim_address(x)\n",
    ")\n",
    "\n",
    "profile_valid_addresses.drop(\n",
    "    profile_valid_addresses[\n",
    "        profile_valid_addresses.truncated_addresses==\"171 Beth Eden St,CA\"].index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def locate_coordinate(in_address):\n",
    "    sleep(1)\n",
    "    coordinates_=geocode(in_address) \n",
    "    if coordinates_:\n",
    "        return (in_address, \n",
    "                (coordinates_[0], coordinates_[1])\n",
    "               )\n",
    "    else:\n",
    "        print(f\"bad address: {in_address}\")\n",
    "geocode = RateLimiter(LOCATOR.geocode, \n",
    "                      min_delay_seconds=1)\n",
    "coordinates=[locate_coordinate(line) \n",
    "             for line \n",
    "             in list(profile_valid_addresses.truncated_addresses)]\n",
    "coordinates=[file for file in coordinates if file] \n",
    "coordinates_with_addresses=pd.DataFrame(coordinates).rename({0:\"truncated_addresses\", \n",
    "                                  1:\"address_coordinates\"}, \n",
    "                                 axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{APP_DUMPATH}/geo-coordinates-listings.pkd\", \"wb\") as writefile:\n",
    "    dill.dump(coordinates, writefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{APP_DUMPATH}/geo-coordinates-listings.pkd\", \"rb\") as readfile:\n",
    "    coordinates = dill.load(readfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspections=list(coordinates_with_addresses.truncated_addresses)\n",
    "profile_valid_addresses=profile_valid_addresses[\n",
    "    profile_valid_addresses.truncated_addresses.isin(inspections)]\n",
    "\n",
    "profile_valid_coordinates= (\n",
    "    pd\n",
    "    .merge(\n",
    "        coordinates_with_addresses,\n",
    "        profile_valid_addresses,\n",
    "        on=\"truncated_addresses\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_valid_coordinates[\"coordinates\"]=(profile_valid_coordinates \n",
    "                                          .address_coordinates \n",
    "                                          .apply(lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_derive_coordinates(in_profile):\n",
    "    profile=in_profile[~in_profile\n",
    "                       .addresses\n",
    "                       .isna()]\n",
    "\n",
    "    num_address_tokens=(profile\n",
    "     .addresses\n",
    "     .apply(\n",
    "         lambda x: re.split(r\"\\,|MLS\", x))\n",
    "     .str.len())\n",
    "\n",
    "    profile_valid_addresses=profile[num_address_tokens>2] \n",
    "\n",
    "    profile_valid_addresses['truncated_addresses']=(profile_valid_addresses \n",
    "                                                    .addresses.apply(\n",
    "                                                        lambda x: trim_address(x)))\n",
    "\n",
    "    (profile_valid_addresses\n",
    "     .drop(\n",
    "         profile_valid_addresses[\n",
    "             profile_valid_addresses\n",
    "             .truncated_addresses==\"171 Beth Eden St,CA\"]\n",
    "         .index))\n",
    "\n",
    "    geocode = RateLimiter(\n",
    "                  LOCATOR.geocode, \n",
    "                  min_delay_seconds=1)\n",
    "    \n",
    "    coordinates=[locate_coordinate(line) \n",
    "                 for line \n",
    "                 in list(profile_valid_addresses.truncated_addresses)]\n",
    "    coordinates=[file for file in coordinates if file] \n",
    "    coordinates_with_addresses=pd.DataFrame(coordinates).rename({0:\"truncated_addresses\", \n",
    "                                      1:\"address_coordinates\"}, \n",
    "                                     axis=1)\n",
    "    inspections=list(coordinates_with_addresses.truncated_addresses)\n",
    "    profile_valid_addresses=profile_valid_addresses[\n",
    "        profile_valid_addresses.truncated_addresses.isin(inspections)]\n",
    "\n",
    "    profile_valid_coordinates= (\n",
    "        pd\n",
    "        .merge(\n",
    "            coordinates_with_addresses,\n",
    "            profile_valid_addresses,\n",
    "            on=\"truncated_addresses\"))\n",
    "\n",
    "    profile_valid_coordinates[\"coordinates\"]=(profile_valid_coordinates \n",
    "                                              .address_coordinates \n",
    "                                              .apply(lambda x: x[1]))\n",
    "    return profile_valid_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_and_derive_coordinates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{APP_DUMPATH}/listing-profile-with-tags-coordinates-and-addresses\", \"wb\") as writefile:\n",
    "    dill.dump(profile_valid_coordinates.to_json(), writefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{APP_DUMPATH}/listing-profile-with-tags-coordinates-and-addresses\", \"rb\") as readfile:\n",
    "    profile_valid_coordinates=read_json(dill.load(readfile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_map_with_house_listings(in_property_profile):\n",
    "    SF_CITYHALL=\"301 Van Ness Ave, San Francisco, CA 94102\"\n",
    "    displayer_=folium.Map(location=list(LOCATOR.geocode(SF_CITYHALL)[1]),\n",
    "                          zoom_start=6)\n",
    "\n",
    "    (in_property_profile\n",
    "     .apply(lambda x: \n",
    "            mark_location_on_map(\n",
    "                displayer_,\n",
    "                in_location=x[\"address_coordinates\"][0],\n",
    "                in_tag=x[\"tags\"],\n",
    "                in_image_source=x[\"images\"],\n",
    "                in_coordinates=x[\"coordinates\"]), \n",
    "            axis=1))\n",
    "    return displayer_.json\n",
    "map_display=plot_map_with_house_listings(profile_valid_coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "map_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SF_CITYHALL=\"301 Van Ness Ave, San Francisco, CA 94102\"\n",
    "LOCATOR.geocode(SF_CITYHALL)[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
